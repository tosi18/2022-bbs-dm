{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/w4bo/2022-bbs-dm/blob/main/notebooks/02-Housing-DataPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d876ba7",
      "metadata": {
        "id": "4d876ba7"
      },
      "source": [
        "# Pandas, Data Wrangling in Python\n",
        "\n",
        "Importing the data from a csv file.\n",
        "Check also: https://www.kaggle.com/camnugent/california-housing-prices\n",
        "\n",
        "The data contains information from the 1990 California census. It does provide an accessible introductory dataset for teaching people about the basics of machine learning.\n",
        "\n",
        "From https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\n",
        "- This data has metrics such as the population, median income, median housing price, and so on for each block group in California.\n",
        "- Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). We will just call them “districts” for short\n",
        "- The goal is to build a model to predict the median housing price in any district, given all the other metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case you need help with preprocessing with Pandas, check:\n",
        "- https://github.com/w4bo/2022-bbs-dm/blob/main/notebooks/01-PandasFundaments.ipynb"
      ],
      "metadata": {
        "id": "2ht7soMiAalj"
      },
      "id": "2ht7soMiAalj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3932b785",
      "metadata": {
        "scrolled": false,
        "id": "3932b785"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "print(np.__version__)\n",
        "print(pd.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80970f2e",
      "metadata": {
        "id": "80970f2e"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv(\"datasets/housing.csv\", delimiter=\",\")\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/w4bo/2022-bbs-dm/main/notebooks/datasets/housing.csv\", delimiter=\",\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb614d01",
      "metadata": {
        "id": "bb614d01"
      },
      "source": [
        "Dataset description\n",
        "\n",
        "1. `longitude`: A measure of how far west a house is; a higher value is farther west\n",
        "2. `latitude`: A measure of how far north a house is; a higher value is farther north\n",
        "3. `housingMedianAge`: Median age of a house within a block; a lower number is a newer building\n",
        "4. `totalRooms`: Total number of rooms within a block\n",
        "5. `totalBedrooms`: Total number of bedrooms within a block\n",
        "6. `population`: Total number of people residing within a block\n",
        "7. `households`: Total number of households, a group of people residing within a home unit, for a block\n",
        "8. `medianIncome`: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
        "9. `medianHouseValue`: Median house value for households within a block (measured in US Dollars)\n",
        "10. `oceanProximity`: Location of the house w.r.t ocean/sea"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "Zvpap_38Lot8"
      },
      "id": "Zvpap_38Lot8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85441c0b",
      "metadata": {
        "scrolled": true,
        "id": "85441c0b"
      },
      "outputs": [],
      "source": [
        "df.info() # show some statistics on the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048cadf1",
      "metadata": {
        "id": "048cadf1"
      },
      "source": [
        "#### Memory usage\n",
        "\n",
        "What if I change float64 to float32?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f1e7c90",
      "metadata": {
        "id": "8f1e7c90"
      },
      "outputs": [],
      "source": [
        "dff = df.copy(deep=True) # copy the dataframe\n",
        "for x in df.columns: # iterate over the columns\n",
        "    if dff[x].dtype == 'float64': # if the column has type `float64`\n",
        "        dff[x] = dff[x].astype('float32') # ... change it to `float32`\n",
        "dff.info() # show some statistics on the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6a24b3a",
      "metadata": {
        "id": "e6a24b3a"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ef54881",
      "metadata": {
        "id": "2ef54881"
      },
      "source": [
        "#### Missing values\n",
        "There are some missing values for `total_bedrooms`. What should we do?\n",
        "\n",
        "Most Machine Learning algorithms cannot work with missing features. We have three options:\n",
        "- Get rid of the corresponding districts (i.e., drop the rows)\n",
        "    - `df.dropna(subset=[\"total_bedrooms\"])`\n",
        "- Get rid of the whole attribute (i.e., drop the columns)\n",
        "    - `df.drop(\"total_bedrooms\", axis=1)`\n",
        "- Set the values to some value (zero, the mean, the median, etc.)\n",
        "    - `df[\"total_bedrooms\"].fillna(df[\"total_bedrooms\"].median())`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef1d868f",
      "metadata": {
        "id": "ef1d868f"
      },
      "source": [
        "#### Non-numeric attributes\n",
        "`ocean_proximity` is a text attribute so we cannot compute its median. Some options:\n",
        "- Get rid of the whole attribute. (`df.drop(\"ocean_proximity\", axis=1`)\n",
        "- Change from categorical to ordinal (e.g., `NEAR BAY` = 0, `INLAND` = 1)\n",
        "    - Can foresee any problem in this?\n",
        "    - ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as “bad”, “average”, “good”, “excellent”), but it is obviously not the case for the ocean_proximity column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1). \n",
        "- Change from categorical to one hot encoding\n",
        "    - To fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is “<1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called dummy attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3297de8e",
      "metadata": {
        "id": "3297de8e"
      },
      "outputs": [],
      "source": [
        "df[\"ocean_proximity\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ocean_proximity\"].hist()"
      ],
      "metadata": {
        "id": "U1AUf_x3e2F_"
      },
      "id": "U1AUf_x3e2F_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8d0ee037",
      "metadata": {
        "id": "8d0ee037"
      },
      "source": [
        "Change from categorical to ordinal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0efd3a04",
      "metadata": {
        "id": "0efd3a04"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "y = ordinal_encoder.fit_transform(df[[\"ocean_proximity\"]])\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe14dd8c",
      "metadata": {
        "id": "fe14dd8c"
      },
      "source": [
        "From categorical to one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3db3bf4",
      "metadata": {
        "id": "d3db3bf4"
      },
      "outputs": [],
      "source": [
        "y = pd.get_dummies(df[\"ocean_proximity\"], prefix='ocean_proximity')\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4969d930",
      "metadata": {
        "id": "4969d930"
      },
      "source": [
        "### Visualization\n",
        "\n",
        "Visualizing attribute distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "570f14ff",
      "metadata": {
        "scrolled": true,
        "id": "570f14ff"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "df.hist(bins=50, figsize=(20,15))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ddb3f44",
      "metadata": {
        "id": "5ddb3f44"
      },
      "source": [
        "Open questions:\n",
        "\n",
        "- `median_income` should be in dollars. However, it has a strange range. Why? \"you are told that the data has been scaled and capped at 15 (actually 15.0001) for higher median incomes, and at 0.5 (actually 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars. The numbers represent roughly tens of thousands of dollars\"\n",
        "- `housing_median_age` and `median_house_value` are capped. As to `median_house_value`, this is a serious problem since it is your target attribute (your labels). Your Machine Learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system’s output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond 500,000USD, then you have mainly two options: (a) collect proper labels for the districts whose labels were capped, (b) remove those districts from the training set.\"\n",
        "- These attributes have very different scales. Should we scale them?\n",
        "- Many histograms are tail heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfeb8426",
      "metadata": {
        "id": "bfeb8426"
      },
      "source": [
        "#### Are the relationships between variables?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c25e557",
      "metadata": {
        "id": "9c25e557"
      },
      "source": [
        "Create a grid of Axes such that each numeric variable in data will by shared across the y-axes across a single row and the x-axes across a single column. The diagonal plots are treated differently: a univariate distribution plot is drawn to show the marginal distribution of the data in each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61702f49",
      "metadata": {
        "id": "61702f49"
      },
      "outputs": [],
      "source": [
        "tmp = df[[\"median_income\", \"housing_median_age\", \"median_house_value\", \"households\", \"population\", \"total_rooms\"]]\n",
        "# sns.pairplot(tmp.sample(n=1000, random_state=42), hue=\"median_house_value\", markers='+')\n",
        "sns.pairplot(tmp.sample(n=1000, random_state=42), markers='+')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1aee617",
      "metadata": {
        "id": "c1aee617"
      },
      "source": [
        "Check correlations and intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79b0a27b",
      "metadata": {
        "id": "79b0a27b"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "rho = df.corr(method ='pearson')\n",
        "pval = df.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
        "p = pval.applymap(lambda x: ''.join(['*' for t in [0.01, 0.05, 0.1] if x <= t]))\n",
        "rho.round(2).astype(str) + p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae54cf98",
      "metadata": {
        "id": "ae54cf98"
      },
      "outputs": [],
      "source": [
        "min_corr = 0.3\n",
        "kot = rho[(abs(rho) >= min_corr) & (rho < 1)]\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(kot, cmap=sns.color_palette(\"coolwarm\", as_cmap=True)) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69801a8d",
      "metadata": {
        "id": "69801a8d"
      },
      "source": [
        "#### Scaling attributes\n",
        "\n",
        "Attributes have very different scales. Should we scale them?\n",
        "\n",
        "\n",
        "- Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n",
        "- Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e312e6b6",
      "metadata": {
        "id": "e312e6b6"
      },
      "source": [
        "#### Min-max normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6974275",
      "metadata": {
        "id": "e6974275"
      },
      "outputs": [],
      "source": [
        "num_df = df.drop(columns=['ocean_proximity', 'median_house_value'])\n",
        "normalized_df = (num_df - num_df.min()) / (num_df.max() - num_df.min())\n",
        "normalized_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0195d8f8",
      "metadata": {
        "id": "0195d8f8"
      },
      "source": [
        "#### Standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39239dcf",
      "metadata": {
        "id": "39239dcf"
      },
      "outputs": [],
      "source": [
        "num_df = df.drop(columns=['ocean_proximity', 'median_house_value'])\n",
        "normalized_df = (num_df - num_df.mean()) / num_df.std()\n",
        "normalized_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "681562cb",
      "metadata": {
        "id": "681562cb"
      },
      "source": [
        "#### Hands on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef4ed12b",
      "metadata": {
        "id": "ef4ed12b"
      },
      "outputs": [],
      "source": [
        "num_df = df.copy(deep=True).drop(columns=[\"ocean_proximity\"]) # do not change this line\n",
        "\n",
        "# Filling in (i.e., impute) missing values with the median value \n",
        "num_df[\"total_bedrooms\"] = 1 # change `1` with the proper solution \n",
        "\n",
        "# Add a new column: population_per_household = population / households\n",
        "num_df[\"population_per_household\"] = 1 # change `1` with the proper solution \n",
        "\n",
        "# Add a new column: rooms_per_household = total_rooms / households\n",
        "num_df[\"rooms_per_household\"] = 1 # change `1` with the proper solution \n",
        "\n",
        "# Add a new column: bedrooms_per_room = total_bedrooms / total_rooms\n",
        "num_df[\"bedrooms_per_room\"] = 1 # change `1` with the proper solution \n",
        "\n",
        "# Apply standardization to all the numeric columns\n",
        "num_df = pd.DataFrame() # change `pd.DataFrame()` with the proper solution \n",
        "\n",
        "# One hot encode `ocean_proximity` since it is a categorical attribute \n",
        "cat_df = pd.DataFrame() # change `pd.DataFrame()` with the proper solution (hint: pd.get_dummies)\n",
        "\n",
        "clean_df = pd.concat([num_df, cat_df], axis=1) # do not change this line\n",
        "clean_df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "name": "02-Housing-DataPreprocessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}